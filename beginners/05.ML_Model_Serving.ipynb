{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have a working Neural Network classifier. \n",
    "\n",
    "Next step will be to save this model to the [model catalog](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/manage-models.htm). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "import json\n",
    "import logging\n",
    "import oci\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import string\n",
    "import tempfile\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model import ADSModel\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from oci.data_science import models\n",
    "from ads.model.deployment import ModelDeployer, ModelDeploymentProperties\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings('ignore')\n",
    "ads.set_documentation_mode(False)\n",
    "logging.getLogger('ads').setLevel(level=logging.ERROR)\n",
    "logging.getLogger('ADS').setLevel(level=logging.ERROR)\n",
    "logging.getLogger('ODSC-ModelDeployment').setLevel(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "    MODEL: ./models/neural_network_classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_filepath='./models/{}.pkl'.format('neural_network_classifier')\n",
    "\n",
    "# load model using pickle l from disk\n",
    "print('Loading model ...\\n    MODEL: {}'.format(model_filepath))\n",
    "loaded_model = pickle.load(open(model_filepath, 'rb'))\n",
    "# model = joblib.load(model_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model in the Data Science Model Catalog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the model in the catalog we are going to use the `ads` library and its `prepare_generic_model()` function. That is probably the easiest way to save a model to the catalog. The first step is to create a temporary local directory where we are going to store the model artifact files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ads\n",
    "\n",
    "# Using resource principal to authenticate when using the model catalog:\n",
    "ads.set_auth(auth='resource_principal')\n",
    "compartment_id = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "project_id = os.environ['PROJECT_OCID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os\n",
    "\n",
    "#Replace with your own path: \n",
    "path_to_rf_artifact = f\"./ads\"\n",
    "if not os.path.exists(path_to_rf_artifact):\n",
    "    os.mkdir(path_to_rf_artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model and convert it to an ADSModel object\n",
    "model_ads = ADSModel.from_estimator(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artifact = prepare_generic_model(path_to_rf_artifact, force_overwrite=True, data_science_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./ads/nnc.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(loaded_model, os.path.join(path_to_rf_artifact, \"nnc.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a serialized model object in our artifact directory, the next step is to modify the file `func.py` which contains the definition of the Oracle Functions handler (`handler()`) function. The handler function is the function that is being called by Oracle Functions. \n",
    "\n",
    "#### Adding Loggers to handler() \n",
    "\n",
    "In the cell below I wrote a new version of `func.py`. Executing this cell will overwrite the template ADS provides as part of the model artifact. \n",
    "\n",
    "You should note a couple of differences with respect to the template. First, I import the Python `logging` library and define a couple of loggers: `model-prediction` and `model-input-features`. I am using these two loggers to capture the model predictions and the model input features for each call made to the Function. That is what I need to monitor how my predictions and features distributions are changing over time. Those log entries are captured and stored in the [Logging service](https://docs.cloud.oracle.com/en-us/iaas/Content/Logging/Concepts/loggingoverview.htm#loggingoverview). Second, I added some additional data transformations in `handler()`. You could have achieved a similar outcome by adding those transformations to the body of `predict()` in `score.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./ads/func.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {path_to_rf_artifact}/func.py\n",
    "\n",
    "import io\n",
    "import json\n",
    "\n",
    "from fdk import response\n",
    "import sys\n",
    "sys.path.append('/function')\n",
    "import score\n",
    "import pandas as pd\n",
    "model = score.load_model()\n",
    "\n",
    "# Importing and configuring logging: \n",
    "import logging\n",
    "logging.basicConfig(format='%(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "# configuring logging: \n",
    "# For model predictions: \n",
    "logger_pred = logging.getLogger('model-prediction')\n",
    "logger_pred.setLevel(logging.INFO)\n",
    "# For the input feature vector: \n",
    "logger_input = logging.getLogger('model-input-features')\n",
    "logger_input.setLevel(logging.INFO)\n",
    "\n",
    "def handler(ctx, data: io.BytesIO=None):\n",
    "    try:\n",
    "        input = json.loads(data.getvalue())['input']\n",
    "        logger_input.info(input)\n",
    "        input2 = json.loads(input)\n",
    "        input_df = pd.DataFrame.from_dict(input2)\n",
    "        prediction = score.predict(input_df, model)\n",
    "        logger_pred.info(prediction)\n",
    "    except (Exception, ValueError) as ex:\n",
    "        logger_pred.info(\"prediction fail {}\".format(str(ex)))\n",
    "\n",
    "    return response.Response(\n",
    "        ctx, response_data=json.dumps(\"predictions: {}\".format(prediction)),\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I modify the `requirements.txt` file. ADS generates a template for `requirements.txt` that provides a best guess at the dependencies necessary to build the Oracle Function and run the model. In this case, I modified the template and added dependencies on `scikit-learn` version 0.21.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./ads/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {path_to_rf_artifact}/requirements.txt\n",
    "\n",
    "cloudpickle==1.6\n",
    "pandas==1.1.0\n",
    "numpy==1.18.5\n",
    "fdk==0.1.18\n",
    "scikit-learn==0.23.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am done with the Oracle Functions part. The last thing I need to do is to modify the inference script `score.py` which loads the model to memory and call the `predict()` method of the model object. \n",
    "\n",
    "By default, ADS generates this file assuming that you are using `cloudpickle` to read the model serialized object. In our case, we are using `joblib`. I modified `score.py` to make use of `joblib`. I left the definition of `predict()` intact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./ads/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {path_to_rf_artifact}/score.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "from joblib import load\n",
    "\n",
    "\"\"\"\n",
    "   Inference script. This script is used for prediction by scoring server when schema is known.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads model from the serialized format\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model:  a model instance on which predict API can be invoked\n",
    "    \"\"\"\n",
    "    model_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    contents = os.listdir(model_dir)\n",
    "    model_file_name = \"nnc.joblib\"\n",
    "    # TODO: Load the model from the model_dir using the appropriate loader\n",
    "    # Below is a sample code to load a model file using `cloudpickle` which was serialized using `cloudpickle`\n",
    "    # from cloudpickle import cloudpickle\n",
    "    if model_file_name in contents:\n",
    "        with open(os.path.join(os.path.dirname(os.path.realpath(__file__)), model_file_name), \"rb\") as file:\n",
    "            model = load(file) # Use the loader corresponding to your model file.\n",
    "    else:\n",
    "        raise Exception('{0} is not found in model directory {1}'.format(model_file_name, model_dir))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(data, model=load_model()) -> dict:\n",
    "    \"\"\"\n",
    "    Returns prediction given the model and data to predict\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Model instance returned by load_model API\n",
    "    data: Data format as expected by the predict API of the core estimator. For eg. in case of sckit models it could be numpy array/List of list/Panda DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions: Output from scoring server\n",
    "        Format: { 'prediction': output from `model.predict` method }\n",
    "\n",
    "    \"\"\"\n",
    "    assert model is not None, \"Model is not loaded\"\n",
    "    # X = pd.read_json(io.StringIO(data)) if isinstance(data, str) else pd.DataFrame.from_dict(data)\n",
    "    return { 'prediction': model.predict(data).tolist() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model Artifact before Saving to the Model Catalog \n",
    "\n",
    "It is always a good idea to test your model artifact in your notebook session before saving it to the catalog. Especially if your Oracle Function depends on it. That is exactly what I am doing next.\n",
    "\n",
    "I first modify my Python path and insert the path where the `score.py` module is located. I then import `score` and  call the `predict()` function defined in `score.py`. I load the train dataset and compare the predictions from `predict()` to the `predictions` array I created right after training model. If `load_model()` and `predict()` functions are doing the right thing I should retrieve the same `predictions` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "data = pd.read_csv('./data/f1_df_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()\n",
    "df.podium = df.podium.map(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "train = df[df.season < 2019]\n",
    "X_train = train.drop(['driver', 'podium'], axis = 1)\n",
    "y_train = train.podium\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for circuit in df[df.season == 2019]['round'].unique():\n",
    "\n",
    "    test = df[(df.season == 2019) & (df['round'] == circuit)]\n",
    "    X_test = test.drop(['driver', 'podium'], axis = 1)\n",
    "    y_test = test.podium\n",
    "\n",
    "    #scaling\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two arrays are equal: True\n"
     ]
    }
   ],
   "source": [
    "# add the path of score.py: \n",
    "import sys \n",
    "import numpy as np \n",
    "sys.path.insert(0, path_to_rf_artifact)\n",
    "\n",
    "from score import load_model, predict\n",
    "\n",
    "# Load the model to memory \n",
    "_ = load_model()\n",
    "# make predictions on the training dataset: \n",
    "predictions_test = predict(X_test, _)\n",
    "\n",
    "# comparing the predictions from predict() to the predictions array I created above. \n",
    "print(f\"The two arrays are equal: {np.array_equal(predictions_test['prediction'], y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Artifact Model to Model Catalog of Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_70f2159c-cd1a-4d9e-a6c9-e115e6ec3048.zip\n",
      "Model OCID: ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaaht5jzvaatec52bw4bj3lo3e72rltrymccqohbag6ixrd5ldckysq\n"
     ]
    }
   ],
   "source": [
    "# Store the model in the Model Catalog\n",
    "mc_model = artifact.save(project_id=project_id, compartment_id=compartment_id,\n",
    "                               display_name=\"Neural Network Classifier\",\n",
    "                               description=\"A F1 Neural Network Classifier\",\n",
    "                               ignore_pending_changes=True)\n",
    "shutil.rmtree(path_to_rf_artifact)\n",
    "model_id = mc_model.id\n",
    "print(f\"Model OCID: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dataexpl_p37_cpu_v1]",
   "language": "python",
   "name": "conda-env-dataexpl_p37_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
